import streamlit as st
import os
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.llms import Ollama
from langchain.chains import RetrievalQA

# --- C√ÄI ƒê·∫∂T CHUNG CHO TRANG ---
st.set_page_config(page_title="ChatPDF C·ª•c B·ªô", layout="wide")

# --- H√ÄM X·ª¨ L√ù ---

@st.cache_resource
def create_vector_store(file_path, _embeddings):
    """T·∫£i, ph√¢n ƒëo·∫°n v√† t·∫°o vector store t·ª´ file PDF."""
    loader = PyPDFLoader(file_path)
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    texts = text_splitter.split_documents(documents)
    vectorstore = Chroma.from_documents(texts, _embeddings)
    return vectorstore

@st.cache_data
def get_file_name(uploaded_file):
    """L·∫•y t√™n file ƒë√£ t·∫£i l√™n."""
    return uploaded_file.name if uploaded_file else "Ch∆∞a c√≥ file n√†o"

# --- GIAO DI·ªÜN THANH B√äN (SIDEBAR) ---

with st.sidebar:
    st.header("‚öôÔ∏è B·∫£ng ƒëi·ªÅu khi·ªÉn")
    st.subheader("1. T·∫£i l√™n t√†i li·ªáu c·ªßa b·∫°n")
    uploaded_file = st.file_uploader("Ch·ªçn m·ªôt file PDF", type="pdf", label_visibility="collapsed")

    if uploaded_file:
        # L∆∞u file t·∫°m th·ªùi ƒë·ªÉ LangChain c√≥ th·ªÉ ƒë·ªçc
        temp_dir = "temp_docs"
        if not os.path.exists(temp_dir):
            os.makedirs(temp_dir)
        temp_file_path = os.path.join(temp_dir, uploaded_file.name)

        with open(temp_file_path, "wb") as f:
            f.write(uploaded_file.getvalue())

        st.success(f"ƒê√£ t·∫£i l√™n: {uploaded_file.name}")

    st.subheader("2. Ch·ªçn m√¥ h√¨nh")
    # Gi·∫£ l·∫≠p vi·ªác ch·ªçn m√¥ h√¨nh. B·∫°n c√≥ th·ªÉ m·ªü r·ªông ƒë·ªÉ th·ª±c s·ª± thay ƒë·ªïi m√¥ h√¨nh
    # ƒê·∫£m b·∫£o b·∫°n ƒë√£ ch·∫°y `ollama pull <model_name>` cho c√°c m√¥ h√¨nh n√†y
    model_name = st.selectbox(
        "Ch·ªçn m√¥ h√¨nh LLM (ch·∫°y b·∫±ng Ollama):",
        ("llama3", "mistral", "gemma"),
        key="llm_model"
    )

    if st.button("B·∫Øt ƒë·∫ßu cu·ªôc tr√≤ chuy·ªán m·ªõi", type="primary"):
        # X√≥a l·ªãch s·ª≠ chat v√† vectorstore ƒë·ªÉ b·∫Øt ƒë·∫ßu l·∫°i
        st.session_state.messages = []
        st.session_state.vectorstore = None
        # X√≥a file t·∫°m n·∫øu c√≥
        if 'temp_file_path' in locals() and os.path.exists(temp_file_path):
             os.remove(temp_file_path)
        st.success("ƒê√£ s·∫µn s√†ng cho cu·ªôc tr√≤ chuy·ªán m·ªõi!")


# --- KH·ªûI T·∫†O STATE V√Ä C√ÅC BI·∫æN C·∫¶N THI·∫æT ---

# Kh·ªüi t·∫°o session state ƒë·ªÉ l∆∞u tin nh·∫Øn
if "messages" not in st.session_state:
    st.session_state.messages = []

# Kh·ªüi t·∫°o vector store trong session state
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None

# Kh·ªüi t·∫°o c√°c m√¥ h√¨nh t·ª´ Ollama
try:
    embeddings = OllamaEmbeddings(model=st.session_state.get('llm_model', 'llama3'))
    llm = Ollama(model=st.session_state.get('llm_model', 'llama3'))
except Exception as e:
    st.error(f"Kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi Ollama. H√£y ƒë·∫£m b·∫£o Ollama ƒëang ch·∫°y.\nL·ªói: {e}")
    st.stop()


# X·ª≠ l√Ω file khi ƒë∆∞·ª£c t·∫£i l√™n v√† vectorstore ch∆∞a ƒë∆∞·ª£c t·∫°o
if uploaded_file and st.session_state.vectorstore is None:
    with st.spinner("ƒêang x·ª≠ l√Ω t√†i li·ªáu... Vi·ªác n√†y c√≥ th·ªÉ m·∫•t m·ªôt l√∫c."):
        st.session_state.vectorstore = create_vector_store(temp_file_path, embeddings)


# --- GIAO DI·ªÜN CH√çNH (MAIN CHAT AREA) ---

st.title("üí¨ ChatPDF C·ª•c b·ªô")
st.caption(f"ƒêang s·ª≠ d·ª•ng m√¥ h√¨nh: {st.session_state.get('llm_model', 'llama3')}")


# Hi·ªÉn th·ªã l·ªãch s·ª≠ tin nh·∫Øn
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# X·ª≠ l√Ω input t·ª´ ng∆∞·ªùi d√πng
if prompt := st.chat_input("H·ªèi b·∫•t c·ª© ƒëi·ªÅu g√¨ v·ªÅ t√†i li·ªáu c·ªßa b·∫°n..."):
    # Th√™m tin nh·∫Øn c·ªßa ng∆∞·ªùi d√πng v√†o l·ªãch s·ª≠
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # T·∫°o v√† hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi c·ªßa AI
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        with st.spinner("AI ƒëang suy nghƒ©..."):
            if st.session_state.vectorstore is not None:
                # T·∫°o chu·ªói truy v·∫•n n·∫øu c√≥ t√†i li·ªáu
                qa_chain = RetrievalQA.from_chain_type(
                    llm=llm,
                    chain_type="stuff",
                    retriever=st.session_state.vectorstore.as_retriever()
                )
                response = qa_chain.invoke(prompt)
                full_response = response.get('result', "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y c√¢u tr·∫£ l·ªùi trong t√†i li·ªáu.")
            else:
                # Tr·∫£ l·ªùi chung n·∫øu ch∆∞a c√≥ t√†i li·ªáu
                full_response = "Vui l√≤ng t·∫£i l√™n m·ªôt file PDF ·ªü thanh b√™n tr√°i ƒë·ªÉ t√¥i c√≥ th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa b·∫°n."

        message_placeholder.markdown(full_response)

    # Th√™m c√¢u tr·∫£ l·ªùi c·ªßa AI v√†o l·ªãch s·ª≠
    st.session_state.messages.append({"role": "assistant", "content": full_response})
